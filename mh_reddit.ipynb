{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "import numpy\n",
    "import collections\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from textstat.textstat import textstat\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "counter_sw = collections.Counter()\n",
    "counter_mh = collections.Counter()\n",
    "pattern = re.compile('\\w+')  # regular expression for word\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# load post, comment from json files\n",
    "data = []\n",
    "with open(\"post_2014.json\",'r',encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "comments = []\n",
    "with open(\"comment_2014.json\",'r',encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        comments.append(json.loads(line))\n",
    "\n",
    "def getTerms(sentences):\n",
    "    tokens = nltk.wordpunct_tokenize(sentences)\n",
    "    words = [w.lower() for w in tokens if w.isalnum()]\n",
    "    return words\n",
    "\n",
    "def getTreatmentToken():\n",
    "    with open('token2.txt') as f:\n",
    "        tokens = f.read().splitlines()\n",
    "    f.close()\n",
    "    token_lst = []\n",
    "    for token in tokens:\n",
    "        tokenized = getTerms(token)\n",
    "        if len(tokenized) > 1:\n",
    "            tup = (tokenized[0],tokenized[1])\n",
    "            token_lst.append(tup)\n",
    "        else:\n",
    "            token_lst.append(token)\n",
    "    return token_lst\n",
    "\n",
    "treatmentToken = getTreatmentToken()\n",
    "\n",
    "# metadata of post, comments\n",
    "author = []\n",
    "\n",
    "sw_post = []\n",
    "mh_post = []\n",
    "\n",
    "length_sw_post = []\n",
    "length_mh_post = []\n",
    "\n",
    "sw_polar_score = []\n",
    "mh_polar_score = []\n",
    "\n",
    "neg_sw_score = []\n",
    "pos_sw_score = []\n",
    "neu_sw_score = []\n",
    "compound_sw_score = []\n",
    "neg_mh_score = []\n",
    "pos_mh_score = []\n",
    "neu_mh_score = []\n",
    "compound_mh_score = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    author.append(data[i][\"author\"])\n",
    "\n",
    "unique_author = list(set(author))\n",
    "\n",
    "# posts from SuicideWatch\n",
    "for i in range(len(data)):\n",
    "    if data[i][\"subreddit\"] == \"SuicideWatch\":\n",
    "        sw_post.append(data[i][\"selftext\"])\n",
    "        \n",
    "# post from other MH subreddit       \n",
    "for i in range(len(data)):\n",
    "    if data[i][\"subreddit\"] != \"SuicideWatch\":\n",
    "        mh_post.append(data[i][\"selftext\"])\n",
    "\n",
    "# calculate length of posts from SW\n",
    "for i in range(len(sw_post)):\n",
    "    length_sw_post.append(len(getTerms(sw_post[i])))\n",
    "    \n",
    "# calculate length of posts from MH\n",
    "for i in range(len(mh_post)):\n",
    "    length_mh_post.append(len(getTerms(mh_post[i])))\n",
    "\n",
    "# calculate Vader polarity score of posts from SW\n",
    "for post in sw_post:\n",
    "    polar_score = analyzer.polarity_scores(post)\n",
    "    sw_polar_score.append(polar_score)\n",
    "\n",
    "# calculate Vader polarity score of posts from MH    \n",
    "for post in mh_post:\n",
    "    polar_score = analyzer.polarity_scores(post)\n",
    "    mh_polar_score.append(polar_score)\n",
    "\n",
    "for score in sw_polar_score:\n",
    "    neg_sw_score.append(score['neg'])\n",
    "    pos_sw_score.append(score['pos'])\n",
    "    neu_sw_score.append(score['neu'])\n",
    "    compound_sw_score.append(score['compound'])\n",
    "    \n",
    "for score in mh_polar_score:\n",
    "    neg_mh_score.append(score['neg'])\n",
    "    pos_mh_score.append(score['pos'])\n",
    "    neu_mh_score.append(score['neu'])\n",
    "    compound_mh_score.append(score['compound'])\n",
    "\n",
    "\n",
    "for post in sw_post:\n",
    "    for word in getTerms(post):\n",
    "        if pattern.match(word) and word not in stopwords:\n",
    "            counter_sw[word.lower()] += 1\n",
    "\n",
    "for post in mh_post:\n",
    "    for word in getTerms(post):\n",
    "        if pattern.match(word) and word not in stopwords:\n",
    "            counter_mh[word.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sw_comments = []\n",
    "mh_comments = []\n",
    "length_sw_comment = []\n",
    "length_mh_comment = []\n",
    "# posts from SuicideWatch\n",
    "for i in range(len(comments)):\n",
    "    if comments[i][\"subreddit\"] == \"SuicideWatch\":\n",
    "        sw_comments.append(comments[i][\"body\"])\n",
    "        \n",
    "# post from other MH subreddit       \n",
    "for i in range(len(comments)):\n",
    "    if comments[i][\"subreddit\"] != \"SuicideWatch\":\n",
    "        mh_comments.append(comments[i][\"body\"])\n",
    "        \n",
    "# calculate length of posts from SW\n",
    "for i in range(len(sw_comments)):\n",
    "    length_sw_comment.append(len(getTerms(sw_comments[i])))\n",
    "    \n",
    "# calculate length of posts from MH\n",
    "for i in range(len(mh_comments)):\n",
    "    length_mh_comment.append(len(getTerms(mh_comments[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts:  80581\n",
      "Number of comments:  409430\n",
      "Number of unique author:  34549\n",
      "Number of SW post:  20025\n",
      "Number of MH post:  60556\n",
      "Number of SW comment:  131330\n",
      "Number of MH comment:  278100\n",
      "Average, median length in SW post:  (250.29538077403245, 165.0)\n",
      "Average, median length in MH post:  (251.02151727326773, 164.0)\n",
      "Average, median length in SW comment:  (57.249836290261172, 26.0)\n",
      "Average, median length in MH comment:  (66.747504494786043, 36.0)\n",
      "Most common 20 unigram in SW post:  [('like', 25290), ('want', 22485), ('know', 21755), ('life', 21213), ('feel', 20429), ('get', 17066), ('even', 15100), ('would', 14854), ('time', 14826), ('people', 14607), ('one', 14156), ('really', 14015), ('going', 11658), ('never', 11482), ('think', 11233), ('friends', 11120), ('go', 11009), ('much', 10039), ('years', 9995), ('help', 9810)]\n",
      "Most common 20 unigram in MH post:  [('like', 87222), ('feel', 75385), ('know', 64496), ('get', 55209), ('want', 52594), ('time', 49249), ('really', 48359), ('life', 47545), ('people', 45693), ('even', 43755), ('would', 40328), ('one', 39295), ('friends', 33830), ('think', 32685), ('go', 32661), ('depression', 32379), ('going', 32089), ('things', 31335), ('never', 31227), ('much', 28790)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of posts: \", len(data))\n",
    "print(\"Number of comments: \", len(comments))\n",
    "print(\"Number of unique author: \",len(unique_author))        \n",
    "print(\"Number of SW post: \", len(sw_post))\n",
    "print(\"Number of MH post: \", len(mh_post))\n",
    "print(\"Number of SW comment: \", len(sw_comments))\n",
    "print(\"Number of MH comment: \", len(mh_comments))\n",
    "print(\"Average, median length in SW post: \", (numpy.mean(length_sw_post), numpy.median(length_sw_post)))\n",
    "print(\"Average, median length in MH post: \", (numpy.mean(length_mh_post), numpy.median(length_mh_post)))\n",
    "print(\"Average, median length in SW comment: \", (numpy.mean(length_sw_comment), numpy.median(length_sw_comment)))\n",
    "print(\"Average, median length in MH comment: \", (numpy.mean(length_mh_comment), numpy.median(length_mh_comment)))\n",
    "# print(\"Average negative polarity score in SW post: \", numpy.mean(neg_sw_score))\n",
    "# print(\"Average negative polarity score in MH post: \", numpy.mean(neg_mh_score))\n",
    "# print(\"Average positive polarity score in SW post: \", numpy.mean(pos_sw_score))\n",
    "# print(\"Average positive polarity score in MH post: \", numpy.mean(pos_mh_score))\n",
    "# print(\"Average neutral polarity score in SW post: \", numpy.mean(neu_sw_score))\n",
    "# print(\"Average neutral polarity score in MH post: \", numpy.mean(neu_mh_score))\n",
    "# print(\"Average compound polarity score in SW post: \", numpy.mean(compound_sw_score))\n",
    "# print(\"Average compound polarity score in MH post: \", numpy.mean(compound_mh_score))\n",
    "print(\"Most common 20 unigram in SW post: \", counter_sw.most_common(20))\n",
    "print(\"Most common 20 unigram in MH post: \", counter_mh.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1392076800)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1407801599)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1407801600)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1415750399)))\n",
    "\n",
    "# get the MH, MH->SW user class\n",
    "def getTargetAuthor(data):\n",
    "    mh_author = []\n",
    "    mh_author_not_in_sw_first_period =[]\n",
    "    sw_author_in_first_period = []\n",
    "    sw_author_in_second_period = []\n",
    "    mh = []\n",
    "    mh_sw = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if (data[i][\"subreddit\"] == \"SuicideWatch\") and (int(data[i][\"created_utc\"]) in range(1392076800, 1407801599)):\n",
    "            sw_author_in_first_period.append(data[i][\"author\"])\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if (data[i][\"subreddit\"] == \"SuicideWatch\") and (int(data[i][\"created_utc\"]) in range(1407801600, 1415750399)):\n",
    "            sw_author_in_second_period.append(data[i][\"author\"])\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if data[i][\"subreddit\"] != \"SuicideWatch\":\n",
    "            mh_author.append(data[i][\"author\"])\n",
    "\n",
    "\n",
    "    for author in mh_author:\n",
    "        if author not in sw_author_in_first_period:\n",
    "            mh_author_not_in_sw_first_period.append(author)\n",
    "\n",
    "    for author in mh_author_not_in_sw_first_period:\n",
    "        if author in sw_author_in_second_period:\n",
    "            mh_sw.append(author)\n",
    "        else:\n",
    "            mh.append(author)\n",
    "\n",
    "    mh_sw = list(set(mh_sw))\n",
    "    mh = list(set(mh))\n",
    "    \n",
    "    return mh, mh_sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.79\n",
      "7.2\n",
      "6.0\n",
      "6.78\n",
      "8.52\n",
      "18\n",
      "8.125\n",
      "19.260845070422533\n",
      "6th and 7th grade\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#print(min(comment_utc))\n",
    "#print(max(comment_utc))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1459469045)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1493596740)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1493596743)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1485867600)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1485867599)))\n",
    "#print(time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.gmtime(1459469009)))\n",
    "# print(len(mh_author_not_in_sw_first_period))\n",
    "# print(len(sw_author_in_first_period))\n",
    "# print(len(sw_author_in_second_period))\n",
    "# print(len(mh))\n",
    "# print(len(mh_sw))\n",
    "\n",
    "text = \"Not just in terms of looks, or the burden I am, but a true monster, like, a truly despicable human being. I feel as if I'm depraved and evil, that my thoughts are twisted and psychotic, and I'm honestly scared of myself. I don't know how much, if any control I have over myself. Honestly, it'd be best for everyone if I was just killed, best for me, and everyone else.\"\n",
    "analyzed = analyzer.polarity_scores(text)\n",
    "print(textstat.flesch_reading_ease(text))\n",
    "print(textstat.automated_readability_index(text))\n",
    "print(textstat.smog_index(text))\n",
    "print(textstat.coleman_liau_index(text))\n",
    "print(textstat.dale_chall_readability_score(text))\n",
    "print(textstat.difficult_words(text))\n",
    "print(textstat.linsear_write_formula(text))\n",
    "print(textstat.gunning_fog(text))\n",
    "print(textstat.text_standard(text))\n",
    "print(type(analyzed['neu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# return post of sampled set of MH user\n",
    "def getSamplePosts(data, comments, mh, mh_sw):\n",
    "    \n",
    "    mh_sw_post = []     # list of tuple (author, post)\n",
    "    mh_sample_post = [] # list of tuple (author, post)\n",
    "    mh_sw_comment = []     # list of tuple (author, post)\n",
    "    mh_sample_comment = [] # list of tuple (author, post)\n",
    "    \n",
    "    #toggle this comment to switch to 'realistic' setting\n",
    "    #mh_sample = mh\n",
    "    \n",
    "    mh_sample = random.sample(mh,len(mh_sw))\n",
    "    \n",
    "#     for i in range(len(data)):\n",
    "#         for author in mh_sample:\n",
    "#             if data[i][\"author\"] == author and data[i][\"selftext\"] != '[deleted]':\n",
    "#                 mh_sample_post.append((author, data[i][\"selftext\"], data[i][\"title\"], data[i][\"score\"], data[i][\"num_comments\"]))\n",
    "#         for author in mh_sw:\n",
    "#             if data[i][\"author\"] == author and data[i][\"selftext\"] != '[deleted]':\n",
    "#                 mh_sw_post.append((author, data[i][\"selftext\"], data[i][\"title\"], data[i][\"score\"], data[i][\"num_comments\"]))\n",
    "    \n",
    "#     for i in range(len(comments)):\n",
    "#         for author in mh_sample:\n",
    "#             if comments[i][\"author\"] == author and comments[i][\"body\"] != '[deleted]':\n",
    "#                 mh_sample_comment.append((author, comments[i][\"body\"], comments[i][\"score\"]))\n",
    "#         for author in mh_sw:\n",
    "#             if comments[i][\"author\"] == author and comments[i][\"body\"] != '[deleted]':\n",
    "#                 mh_sw_comment.append((author, comments[i][\"body\"], comments[i][\"score\"]))\n",
    "                \n",
    "    for i in range(len(data)):\n",
    "        for author in mh_sample:\n",
    "            if data[i][\"author\"] == author and data[i]['selftext'] != '' and data[i]['selftext'] != '[deleted]':\n",
    "                mh_sample_post.append((author, data[i][\"selftext\"], data[i][\"title\"], data[i][\"score\"], data[i][\"num_comments\"]))\n",
    "        for author in mh_sw:\n",
    "            if data[i][\"author\"] == author and data[i]['selftext'] != '' and data[i]['selftext'] != '[deleted]':\n",
    "                mh_sw_post.append((author, data[i][\"selftext\"], data[i][\"title\"], data[i][\"score\"], data[i][\"num_comments\"]))\n",
    "    \n",
    "    for i in range(len(comments)):\n",
    "        for author in mh_sample:\n",
    "            if comments[i][\"author\"] == author and comments[i]['body'] != '' and comments[i]['body'] != '[deleted]':\n",
    "                mh_sample_comment.append((author, comments[i][\"body\"], comments[i][\"score\"]))\n",
    "        for author in mh_sw:\n",
    "            if comments[i][\"author\"] == author and comments[i]['body'] != '' and comments[i]['body'] != '[deleted]':\n",
    "                mh_sw_comment.append((author, comments[i][\"body\"], comments[i][\"score\"]))\n",
    "                \n",
    "    return mh_sw_post, mh_sample_post, mh_sample, mh_sw_comment, mh_sample_comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# prepare a dataset with general feature like faction of pronoun, noun or reading ease\n",
    "def prepareLinguisticDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post):\n",
    "    \n",
    "    dataset = [] # list for feature of MH->SW sample\n",
    "    dataset_2 = []  # list for feature of MH sample\n",
    "    \n",
    "    # iterate through MH->SW users\n",
    "    for author in mh_sw:\n",
    "        \n",
    "        difficult_word = 0 # difficult words is words have more than 3 syllables\n",
    "        reading_ease = 0   # Flesch reading easy index: from 0-100, the higher index the easier text to read\n",
    "        pron_fraction = 0\n",
    "        noun_fraction = 0\n",
    "        verb_fraction = 0\n",
    "        adv_fraction = 0\n",
    "        readability_index = 0\n",
    "        num_post = 0\n",
    "        \n",
    "        # iterate through posts of MH->SW users\n",
    "        for tup in mh_sw_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                num_pron = 0\n",
    "                num_noun = 0\n",
    "                num_adv = 0\n",
    "                num_verb = 0\n",
    "                tagged = nltk.pos_tag(getTerms(tup[1]), tagset=\"universal\")\n",
    "                for word_tag in tagged:\n",
    "                    if word_tag[1] == 'PRON':\n",
    "                        num_pron += 1\n",
    "                    if word_tag[1] == 'NOUN':\n",
    "                        num_noun += 1\n",
    "                    if word_tag[1] == 'VERB':\n",
    "                        num_verb += 1\n",
    "                    if word_tag[1] == 'ADV':\n",
    "                        num_adv += 1\n",
    "                try:\n",
    "                    pron_fraction += num_pron/len(tagged)\n",
    "                except ZeroDivisionError:\n",
    "                    pron_fraction += num_pron/1\n",
    "                    \n",
    "                try:\n",
    "                    verb_fraction += num_verb/len(tagged)\n",
    "                except ZeroDivisionError:\n",
    "                    verb_fraction += num_verb/len(tagged)\n",
    "                    \n",
    "                try:   \n",
    "                    noun_fraction += num_noun/len(tagged)\n",
    "                except ZeroDivisionError:\n",
    "                    noun_fraction += num_noun/1\n",
    "                    \n",
    "                try:\n",
    "                    adv_fraction += num_adv/len(tagged)\n",
    "                except ZeroDivisionError:    \n",
    "                    adv_fraction += num_adv/1\n",
    "\n",
    "                try:\n",
    "                    difficult_word += textstat.difficult_words(tup[1])\n",
    "                except:\n",
    "                    difficult_word += 0\n",
    "\n",
    "                try:\n",
    "                    reading_ease += textstat.flesch_reading_ease(tup[1])\n",
    "                except:\n",
    "                    reading_ease += 90\n",
    "                \n",
    "                try:\n",
    "                    readability_index += textstat.automated_readability_index(tup[1])\n",
    "                except:\n",
    "                    readability_index += 5\n",
    "\n",
    "                num_post += 1\n",
    "        try:\n",
    "            avg_diff_word = difficult_word/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_diff_word = difficult_word/1\n",
    "\n",
    "        try:\n",
    "            avg_read_ease = reading_ease/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_read_ease = reading_ease/1\n",
    "\n",
    "        try:\n",
    "            avg_pron_fraction = pron_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_pron_fraction = pron_fraction/1\n",
    "\n",
    "        try:\n",
    "            avg_noun_fraction = noun_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_noun_fraction = noun_fraction/1\n",
    "        \n",
    "        try:\n",
    "            avg_verb_fraction = verb_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_verb_fraction = verb_fraction/1\n",
    "\n",
    "        try:\n",
    "            avg_adv_fraction = adv_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_adv_fraction = adv_fraction/1\n",
    "            \n",
    "        try:\n",
    "            avg_readability_index = readability_index/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_readability_index = readability_index/1\n",
    "            \n",
    "        dataset.append((avg_diff_word, avg_read_ease, avg_readability_index, avg_pron_fraction, avg_noun_fraction,\\\n",
    "                        avg_verb_fraction, avg_adv_fraction, 1)) \n",
    "                \n",
    "    \n",
    "    # iterate through MH users\n",
    "    for author in mh_sample:\n",
    "        \n",
    "        difficult_word = 0\n",
    "        reading_ease = 0\n",
    "        pron_fraction = 0\n",
    "        noun_fraction = 0\n",
    "        verb_fraction = 0\n",
    "        adv_fraction = 0\n",
    "        readability_index = 0\n",
    "        num_post = 0  \n",
    "        \n",
    "        for tup in mh_sample_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                num_pron = 0\n",
    "                num_noun = 0\n",
    "                num_adv = 0\n",
    "                num_verb = 0\n",
    "                tagged = nltk.pos_tag(getTerms(tup[1]), tagset=\"universal\")\n",
    "                for word_tag in tagged:\n",
    "                    if word_tag[1] == 'PRON':\n",
    "                        num_pron += 1\n",
    "                    if word_tag[1] == 'NOUN':\n",
    "                        num_noun += 1\n",
    "                    if word_tag[1] == 'VERB':\n",
    "                        num_verb += 1\n",
    "                    if word_tag[1] == 'ADV':\n",
    "                        num_adv += 1\n",
    "                try:\n",
    "                    pron_fraction += num_pron/len(tagged)\n",
    "                except ZeroDivisionError:\n",
    "                    pron_fraction += num_pron/1\n",
    "                    \n",
    "                try:   \n",
    "                    noun_fraction += num_noun/len(tagged)\n",
    "                except ZeroDivisionError:\n",
    "                    noun_fraction += num_noun/1\n",
    "                \n",
    "                try:\n",
    "                    verb_fraction += num_verb/len(tagged)\n",
    "                except ZeroDivisionError:\n",
    "                    verb_fraction += num_verb/len(tagged)\n",
    "                    \n",
    "                try:\n",
    "                    adv_fraction += num_adv/len(tagged)\n",
    "                except ZeroDivisionError:    \n",
    "                    adv_fraction += num_adv/1\n",
    "\n",
    "                difficult_word += textstat.difficult_words(tup[1])\n",
    "\n",
    "                try:\n",
    "                    reading_ease += textstat.flesch_reading_ease(tup[1])\n",
    "                except:\n",
    "                    reading_ease += 90\n",
    "                \n",
    "                try:\n",
    "                    readability_index += textstat.automated_readability_index(tup[1])\n",
    "                except:\n",
    "                    readability_index += 5\n",
    "                \n",
    "                num_post += 1\n",
    "                \n",
    "        try:\n",
    "            avg_diff_word = difficult_word/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_diff_word = difficult_word/1\n",
    "\n",
    "        try:\n",
    "            avg_read_ease = reading_ease/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_read_ease = reading_ease/1\n",
    "\n",
    "        try:\n",
    "            avg_pron_fraction = pron_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_pron_fraction = pron_fraction/1\n",
    "\n",
    "        try:\n",
    "            avg_noun_fraction = noun_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_noun_fraction = noun_fraction/1\n",
    "\n",
    "        try:\n",
    "            avg_verb_fraction = verb_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_verb_fraction = verb_fraction/1\n",
    "        try:\n",
    "            avg_adv_fraction = adv_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_adv_fraction = adv_fraction/1\n",
    "        \n",
    "        try:\n",
    "            avg_readability_index = readability_index/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_readability_index = readability_index/1\n",
    "\n",
    "        dataset_2.append((avg_diff_word, avg_read_ease, avg_readability_index, avg_pron_fraction, \\\n",
    "                          avg_noun_fraction, avg_verb_fraction, avg_adv_fraction, 0))\n",
    "\n",
    "    dataset.extend(dataset_2)\n",
    "    #random.shuffle(dataset)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_person_singular = ['i','me','my','mine']\n",
    "first_person_plural = ['we','us','our','ours']\n",
    "second_person_pronoun = ['you','your','yours']\n",
    "third_person_pronoun = ['he','she','it','him','her','his','hers','its','they','them','their','theirs']\n",
    "\n",
    "def prepareInterpersonalDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment):\n",
    "    dataset = []  # list for feature of MH->SW sample\n",
    "    dataset_2 = []  # list for feature of MH sample\n",
    "    \n",
    "    # iterate through MH->SW users\n",
    "    for author in mh_sw:\n",
    "        \n",
    "        sp_fraction = 0\n",
    "        tp_fraction = 0\n",
    "        fp_singular_fraction = 0\n",
    "        fp_plural_fraction = 0\n",
    "        \n",
    "        num_post = 0\n",
    "        for tup in mh_sw_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                sp_pron = 0\n",
    "                tp_pron = 0\n",
    "                fp_singular = 0\n",
    "                fp_plural = 0\n",
    "                #tagged = nltk.pos_tag(getTerms(tup[1]), tagset=\"universal\")\n",
    "                tokenized = getTerms(tup[1])\n",
    "                for word in tokenized:\n",
    "                    if word.lower() in first_person_singular:\n",
    "                        fp_singular += 1\n",
    "                    if word.lower() in first_person_plural:\n",
    "                        fp_plural += 1\n",
    "                    if word.lower() in second_person_pronoun:\n",
    "                        sp_pron += 1\n",
    "                    if word.lower() in third_person_pronoun:\n",
    "                        tp_pron += 1\n",
    "                                \n",
    "                try:\n",
    "                    sp_fraction += sp_pron/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    sp_fraction += sp_pron/1\n",
    "                \n",
    "                try:\n",
    "                    tp_fraction += tp_pron/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    tp_fraction += tp_pron/1\n",
    "                \n",
    "                try:\n",
    "                    fp_singular_fraction += fp_singular/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    fp_singular_fraction += fp_singular/1\n",
    "                \n",
    "                try:\n",
    "                    fp_plural_fraction += fp_plural/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    fp_plural_fraction += fp_plural/1\n",
    "                    \n",
    "                num_post += 1\n",
    "\n",
    "        try:\n",
    "            avg_sp_fraction = sp_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_sp_fraction = sp_fraction/1\n",
    "            \n",
    "        try:\n",
    "            avg_tp_fraction = tp_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_tp_fraction = tp_fraction/1\n",
    "        \n",
    "        try:\n",
    "            avg_fp_singular_fraction = fp_singular_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_fp_singular_fraction = fp_singular_fraction/1\n",
    "            \n",
    "        try:\n",
    "            avg_fp_plural_fraction = fp_plural_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_fp_plural_fraction = fp_plural_fraction/1\n",
    "            \n",
    "        dataset.append((avg_fp_singular_fraction, avg_fp_plural_fraction, avg_sp_fraction, avg_tp_fraction, 1))\n",
    "\n",
    "        \n",
    "    for author in mh_sample:\n",
    "        \n",
    "        sp_fraction = 0\n",
    "        tp_fraction = 0\n",
    "        fp_singular_fraction = 0\n",
    "        fp_plural_fraction = 0\n",
    "        \n",
    "        num_post = 0\n",
    "        for tup in mh_sample_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                sp_pron = 0\n",
    "                tp_pron = 0\n",
    "                fp_singular = 0\n",
    "                fp_plural = 0                \n",
    "                #tagged = nltk.pos_tag(getTerms(tup[1]), tagset=\"universal\")\n",
    "                tokenized = getTerms(tup[1])\n",
    "                for word in tokenized:\n",
    "                    if word.lower() in first_person_singular:\n",
    "                        fp_singular += 1\n",
    "                    if word.lower() in first_person_plural:\n",
    "                        fp_plural += 1\n",
    "                    if word.lower() in second_person_pronoun:\n",
    "                        sp_pron += 1\n",
    "                    if word.lower() in third_person_pronoun:\n",
    "                        tp_pron += 1\n",
    "                                \n",
    "                try:\n",
    "                    sp_fraction += sp_pron/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    sp_fraction += sp_pron/1\n",
    "                \n",
    "                try:\n",
    "                    tp_fraction += tp_pron/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    tp_fraction += tp_pron/1\n",
    "                \n",
    "                try:\n",
    "                    fp_singular_fraction += fp_singular/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    fp_singular_fraction += fp_singular/1\n",
    "                \n",
    "                try:\n",
    "                    fp_plural_fraction += fp_plural/len(tokenized)\n",
    "                except ZeroDivisionError:\n",
    "                    fp_plural_fraction += fp_plural/1\n",
    "                    \n",
    "                num_post += 1\n",
    "\n",
    "        try:\n",
    "            avg_sp_fraction = sp_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_sp_fraction = sp_fraction/1\n",
    "            \n",
    "        try:\n",
    "            avg_tp_fraction = tp_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_tp_fraction = tp_fraction/1\n",
    "        \n",
    "        try:\n",
    "            avg_fp_singular_fraction = fp_singular_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_fp_singular_fraction = fp_singular_fraction/1\n",
    "            \n",
    "        try:\n",
    "            avg_fp_plural_fraction = fp_plural_fraction/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_fp_plural_fraction = fp_plural_fraction/1\n",
    "            \n",
    "        dataset_2.append((avg_fp_singular_fraction, avg_fp_plural_fraction, avg_sp_fraction, avg_tp_fraction, 0)) \n",
    "        \n",
    "    dataset.extend(dataset_2)\n",
    "    #random.shuffle(dataset)\n",
    "    \n",
    "    return dataset        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prepare a dataset with features like number of comments, length of post/title\n",
    "def prepareMetadataDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment):\n",
    "    \n",
    "    dataset = []  # list for feature of MH->SW sample\n",
    "    dataset_2 = []  # list for feature of MH sample\n",
    "    \n",
    "    # iterate through MH->SW users\n",
    "    for author in mh_sw:\n",
    "\n",
    "        post_length = 0\n",
    "        title_length = 0\n",
    "        reddit_score = 0\n",
    "        num_comments_to_post = 0\n",
    "        num_post = 0\n",
    "        num_comment = 0\n",
    "        comment_score = 0\n",
    "        comment_length = 0\n",
    "        \n",
    "        # iterate through posts of MH->SW users\n",
    "        for tup in mh_sw_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                post_length += len(getTerms(tup[1]))\n",
    "                title_length += len(getTerms(tup[2]))\n",
    "                reddit_score += int(tup[3])\n",
    "                num_comments_to_post += int(tup[4])\n",
    "                num_post += 1\n",
    "                \n",
    "        # iterate through comments of MH->SW users       \n",
    "        for tup in mh_sw_comment:\n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                comment_length += len(getTerms(tup[1]))\n",
    "                comment_score += int(tup[2])\n",
    "                num_comment += 1\n",
    "             \n",
    "        \n",
    "        try:\n",
    "            avg_post_length = post_length/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_post_length = post_length/1\n",
    "\n",
    "        try:\n",
    "            avg_title_length = title_length/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_title_length = title_length/1\n",
    "\n",
    "        try:\n",
    "            avg_reddit_score = reddit_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_reddit_score = reddit_score/1\n",
    "\n",
    "        try:\n",
    "            avg_num_comments_to_post = num_comments_to_post/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_num_comments_to_post = num_comments_to_post/1\n",
    "            \n",
    "        try:\n",
    "            avg_comment_length = comment_length/num_comment\n",
    "        except ZeroDivisionError:\n",
    "            avg_comment_length = comment_length/1\n",
    "            \n",
    "        try:\n",
    "            avg_comment_score = comment_score/num_comment\n",
    "        except ZeroDivisionError:\n",
    "            avg_comment_score = comment_score/1\n",
    "\n",
    "        \n",
    "        dataset.append((avg_post_length, avg_title_length,avg_reddit_score, avg_num_comments_to_post,\\\n",
    "                        avg_comment_length, avg_comment_score, num_comment, 1))\n",
    "    \n",
    "    # iterate through MH users\n",
    "    for author in mh_sample:\n",
    "        \n",
    "        post_length = 0\n",
    "        title_length = 0\n",
    "        reddit_score = 0\n",
    "        num_comments_to_post = 0\n",
    "        num_post = 0\n",
    "        num_comment = 0\n",
    "        comment_score = 0\n",
    "        comment_length = 0\n",
    "        \n",
    "        # iterate through posts of MH users\n",
    "        for tup in mh_sample_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                polar_score = analyzer.polarity_scores(tup[1])\n",
    "                post_length += len(getTerms(tup[1]))\n",
    "                title_length += len(getTerms(tup[2]))\n",
    "                reddit_score += int(tup[3])\n",
    "                num_comments_to_post += int(tup[4])\n",
    "                num_post += 1\n",
    "        \n",
    "        # iterate through comments of MH users\n",
    "        for tup in mh_sample_comment:\n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                comment_length += len(getTerms(tup[1]))\n",
    "                comment_score += int(tup[2])\n",
    "                num_comment += 1\n",
    "\n",
    "        try:\n",
    "            avg_post_length = post_length/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_post_length = post_length/1\n",
    "\n",
    "        try:\n",
    "            avg_title_length = title_length/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_title_length = title_length/1\n",
    "\n",
    "        try:\n",
    "            avg_reddit_score = reddit_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_reddit_score = reddit_score/1\n",
    "\n",
    "        try:\n",
    "            avg_num_comments_to_post = num_comments_to_post/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_num_comments_to_post = num_comments_to_post/1\n",
    "            \n",
    "        try:\n",
    "            avg_comment_length = comment_length/num_comment\n",
    "        except ZeroDivisionError:\n",
    "            avg_comment_length = comment_length/1\n",
    "            \n",
    "        try:\n",
    "            avg_comment_score = comment_score/num_comment\n",
    "        except ZeroDivisionError:\n",
    "            avg_comment_score = comment_score/1\n",
    "\n",
    "        dataset_2.append((avg_post_length, avg_title_length, avg_reddit_score, avg_num_comments_to_post,\\\n",
    "                        avg_comment_length, avg_comment_score,num_comment,0))\n",
    "        \n",
    "    dataset.extend(dataset_2)\n",
    "    #random.shuffle(dataset)\n",
    "    \n",
    "    return dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prepare a dataset with feature is score of Vader sentiment in NLTK\n",
    "def prepareSentimentDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment):\n",
    "    \n",
    "    dataset = []  # list for feature of MH->SW sample\n",
    "    dataset_2 = []  # list for feature of MH sample\n",
    "    \n",
    "    # iterate through MH->SW users\n",
    "    for author in mh_sw:\n",
    "        \n",
    "        neg_score = 0\n",
    "        neu_score = 0\n",
    "        pos_score = 0\n",
    "        compound_score = 0\n",
    "        num_post = 0\n",
    "        \n",
    "        # iterate through posts of MH->SW users\n",
    "        for tup in mh_sw_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                polar_score = analyzer.polarity_scores(tup[1])\n",
    "                neg_score += polar_score['neg']\n",
    "                neu_score += polar_score['neu']\n",
    "                pos_score += polar_score['pos']\n",
    "                compound_score += polar_score['compound']\n",
    "                num_post += 1\n",
    "        try:\n",
    "            avg_neg_score = neg_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_neg_score = neg_score/1\n",
    "\n",
    "        try:\n",
    "            avg_neu_score = neu_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_neu_score = neu_score/1\n",
    "\n",
    "        try:\n",
    "            avg_pos_score = pos_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_pos_score = pos_score/1\n",
    "\n",
    "        try:\n",
    "            avg_compound_score = compound_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_compound_score = compound_score/1\n",
    "\n",
    "\n",
    "        dataset.append((avg_neg_score, avg_neu_score, avg_pos_score, avg_compound_score,1))\n",
    "        \n",
    "    # iterate through MH users    \n",
    "    for author in mh_sample:\n",
    "        \n",
    "        neg_score = 0\n",
    "        neu_score = 0\n",
    "        pos_score = 0\n",
    "        compound_score = 0\n",
    "        num_post = 0\n",
    "        \n",
    "        # iterate through posts of MH users\n",
    "        for tup in mh_sample_post:              \n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):\n",
    "                polar_score = analyzer.polarity_scores(tup[1])\n",
    "                neg_score += polar_score['neg']\n",
    "                neu_score += polar_score['neu']\n",
    "                pos_score += polar_score['pos']\n",
    "                compound_score += polar_score['compound']\n",
    "                num_post += 1\n",
    "\n",
    "        try:\n",
    "            avg_neg_score = neg_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_neg_score = neg_score/1\n",
    "\n",
    "        try:\n",
    "            avg_neu_score = neu_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_neu_score = neu_score/1\n",
    "\n",
    "        try:\n",
    "            avg_pos_score = pos_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_pos_score = pos_score/1\n",
    "\n",
    "        try:\n",
    "            avg_compound_score = compound_score/num_post\n",
    "        except ZeroDivisionError:\n",
    "            avg_compound_score = compound_score/1\n",
    "\n",
    "        dataset_2.append((avg_neg_score, avg_neu_score, avg_pos_score, avg_compound_score,0)) \n",
    "    \n",
    "    dataset.extend(dataset_2)\n",
    "    #random.shuffle(dataset)\n",
    "    \n",
    "    return dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepareContentDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment, treatmentToken):\n",
    "    \n",
    "    dataset = []  # list for feature of MH->SW sample\n",
    "    dataset_2 = []  # list for feature of MH sample\n",
    "    \n",
    "    for author in mh_sw:        \n",
    "        \n",
    "        binary_tup = ()\n",
    "        index_lst = []\n",
    "        \n",
    "        for tup in mh_sw_post:\n",
    "            appeared = []\n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):                \n",
    "                unigram_appeared = set(treatmentToken).intersection(getTerms(tup[1]))\n",
    "                bigram = list(nltk.bigrams(getTerms(tup[1])))\n",
    "                bigram_appeared = set(treatmentToken).intersection(bigram)\n",
    "                temp = list(unigram_appeared.union(bigram_appeared))\n",
    "                appeared.extend(temp)                  \n",
    "        \n",
    "            for word in appeared:\n",
    "                index_lst.append(treatmentToken.index(word))\n",
    "        \n",
    "        index_lst = list(set(index_lst))\n",
    "        index_lst.sort()\n",
    "        \n",
    "        for i in range(len(treatmentToken)):\n",
    "            if i in index_lst:\n",
    "                binary_tup = binary_tup + (1,)\n",
    "            else:\n",
    "                binary_tup = binary_tup + (0,)\n",
    "\n",
    "        dataset.append(binary_tup + (1,))\n",
    "        \n",
    "    for author in mh_sample:\n",
    "        \n",
    "        appeared = []\n",
    "        binary_tup = ()\n",
    "        index_lst = []\n",
    "        \n",
    "        for tup in mh_sample_post:\n",
    "            if((tup[0] == author) and (len(getTerms(tup[1])) >= 1)):                \n",
    "                unigram_appeared = set(treatmentToken).intersection(getTerms(tup[1]))\n",
    "                bigram = list(nltk.bigrams(getTerms(tup[1])))\n",
    "                bigram_appeared = set(treatmentToken).intersection(bigram)\n",
    "                temp = list(unigram_appeared.union(bigram_appeared))\n",
    "                appeared.extend(temp)\n",
    "                                         \n",
    "            for word in appeared:\n",
    "                index_lst.append(treatmentToken.index(word))\n",
    "        \n",
    "        index_lst = list(set(index_lst))\n",
    "        index_lst.sort()\n",
    "                            \n",
    "        for i in range(len(treatmentToken)):\n",
    "            if i in index_lst:\n",
    "                binary_tup = binary_tup + (1,)\n",
    "            else:\n",
    "                binary_tup = binary_tup + (0,)\n",
    "\n",
    "        dataset_2.append(binary_tup + (0,))\n",
    "\n",
    "    dataset.extend(dataset_2)\n",
    "\n",
    "    return dataset\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def combineDataset(dataset_linguistic, dataset_interpersonal, dataset_metadata, dataset_sentiment, dataset_content): \n",
    "    dataset_full = []\n",
    "    for i in range(len(dataset_linguistic)):\n",
    "        row = dataset_linguistic[i][0:len(dataset_linguistic[i])-1] + \\\n",
    "              dataset_metadata[i][0:len(dataset_metadata[i])-1] + \\\n",
    "              dataset_interpersonal[i][0:len(dataset_interpersonal[i])-1] + \\\n",
    "            dataset_sentiment[i][0:len(dataset_sentiment[i])-1] + \\\n",
    "                dataset_content[i]\n",
    "        dataset_full.append(row)\n",
    "    return dataset_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mh, mh_sw = getTargetAuthor(data)\n",
    "mh_sw_post, mh_sample_post, mh_sample, mh_sw_comment, mh_sample_comment = getSamplePosts(data, comments, mh, mh_sw)\n",
    "dataset_linguistic = prepareLinguisticDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post)\n",
    "dataset_interpersonal = prepareInterpersonalDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment)\n",
    "dataset_metadata = prepareMetadataDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment)\n",
    "dataset_sentiment = prepareSentimentDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment)\n",
    "dataset_content = prepareContentDataset(mh_sw, mh_sample, mh_sw_post, mh_sample_post, mh_sw_comment, mh_sample_comment, treatmentToken)\n",
    "dataset_full = combineDataset(dataset_linguistic, dataset_interpersonal, dataset_metadata, dataset_sentiment, dataset_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-test for linguistic features:\n",
      "Mean (MH, SW):  56.2272793992 52.617179196\n",
      "(1.11286945945803, 0.26576449835223681)\n",
      "Mean (MH, SW):  66.9645640638 75.5015083001\n",
      "(-2.3553664510276753, 0.018504447218113711)\n",
      "Mean (MH, SW):  8.55422495306 7.14030167813\n",
      "(1.4397044632639591, 0.14995103009452979)\n",
      "Mean (MH, SW):  0.076785509621 0.0802390137692\n",
      "(-1.5292270004686632, 0.12620817925630029)\n",
      "Mean (MH, SW):  0.204161683441 0.208060426166\n",
      "(-0.77941994400627623, 0.43573237919518337)\n",
      "Mean (MH, SW):  0.233315027146 0.249708507797\n",
      "(-3.5836452825216178, 0.00033883216042893371)\n",
      "Mean (MH, SW):  0.0834225367339 0.084553588336\n",
      "(-0.49429376941095438, 0.62109872474103067)\n",
      "Z-test for interaction features:\n",
      "Mean (MH, SW):  278.443300051 265.94557851\n",
      "(0.70034976043848307, 0.48370890304224512)\n",
      "Mean (MH, SW):  7.94463987029 8.31530778689\n",
      "(-0.92215196949545764, 0.35644931227035048)\n",
      "Mean (MH, SW):  6.04557433009 5.78329966546\n",
      "(0.24248470743705972, 0.80840460503222666)\n",
      "Mean (MH, SW):  3.96162655743 5.49700131544\n",
      "(-3.1703488667525574, 0.0015225601449227453)\n",
      "Mean (MH, SW):  40.1774248495 63.4635722241\n",
      "(-4.6062910418638587, 4.0991456924360632e-06)\n",
      "Mean (MH, SW):  0.985311724732 1.51932034582\n",
      "(-4.7629055350752267, 1.9082518608099696e-06)\n",
      "Mean (MH, SW):  4.51612903226 13.1419354839\n",
      "(-3.7003111107865627, 0.00021533531157596935)\n",
      "Z-test for interpersonal features:\n",
      "Mean (MH, SW):  0.094924453866 0.106756558437\n",
      "(-4.7072249337777885, 2.5111177814057542e-06)\n",
      "Mean (MH, SW):  0.00264445768772 0.00242005485114\n",
      "(0.65255163786106096, 0.51404537247789017)\n",
      "Mean (MH, SW):  0.00592199496872 0.00498248415066\n",
      "(1.2317237070360472, 0.2180523109300172)\n",
      "Mean (MH, SW):  0.0321155979728 0.031509442313\n",
      "(0.41093448375104347, 0.68112057594140185)\n",
      "Z-test for sentiment features:\n",
      "Mean (MH, SW):  0.135461404677 0.159000726504\n",
      "(-5.1819154457953438, 2.196187762263099e-07)\n",
      "Mean (MH, SW):  0.683697907493 0.691147476855\n",
      "(-0.65065566810066244, 0.51526878653190544)\n",
      "Mean (MH, SW):  0.11420281874 0.115454406656\n",
      "(-0.31798107576549384, 0.75049929183313779)\n",
      "Mean (MH, SW):  -0.192393654549 -0.362878478674\n",
      "(3.9834697556224574, 6.7916318453457985e-05)\n"
     ]
    }
   ],
   "source": [
    "#from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "print(\"Z-test for linguistic features:\")\n",
    "for i in range(len(dataset_linguistic[1])-1):\n",
    "    linguistic_mh = []\n",
    "    linguistic_mh_sw = []\n",
    "    for j in range(len(dataset_linguistic)):\n",
    "        if dataset_linguistic[j][len(dataset_linguistic[j])-1] == 1:\n",
    "            linguistic_mh_sw.append(dataset_linguistic[j][i])\n",
    "        else:\n",
    "            linguistic_mh.append(dataset_linguistic[j][i])\n",
    "    result = sm.stats.CompareMeans.from_data(linguistic_mh, linguistic_mh_sw).ztest_ind()\n",
    "    #result2 = sm.stats.CompareMeans.from_data(linguistic_mh, linguistic_mh_sw).ttest_ind()\n",
    "    print(\"Mean (MH, SW): \", numpy.mean(linguistic_mh), numpy.mean(linguistic_mh_sw))\n",
    "    print(result)\n",
    "    #print(result2)\n",
    "    \n",
    "print(\"Z-test for interaction features:\")   \n",
    "for i in range(len(dataset_metadata[1])-1):\n",
    "    metadata_mh = []\n",
    "    metadata_mh_sw = []\n",
    "    for j in range(len(dataset_metadata)):\n",
    "        if dataset_metadata[j][len(dataset_metadata[j])-1] == 1:\n",
    "            metadata_mh_sw.append(dataset_metadata[j][i])\n",
    "        else:\n",
    "            metadata_mh.append(dataset_metadata[j][i])\n",
    "    result = sm.stats.CompareMeans.from_data(metadata_mh, metadata_mh_sw).ztest_ind()\n",
    "    #result2 = sm.stats.CompareMeans.from_data(metadata_mh, metadata_mh_sw).ttest_ind()\n",
    "    print(\"Mean (MH, SW): \", numpy.mean(metadata_mh), numpy.mean(metadata_mh_sw))\n",
    "    print(result)\n",
    "    #print(result2)\n",
    "    \n",
    "print(\"Z-test for interpersonal features:\")   \n",
    "for i in range(len(dataset_interpersonal[1])-1):\n",
    "    interpersonal_mh = []\n",
    "    interpersonal_mh_sw = []\n",
    "    for j in range(len(dataset_interpersonal)):\n",
    "        if dataset_interpersonal[j][len(dataset_interpersonal[j])-1] == 1:\n",
    "            interpersonal_mh_sw.append(dataset_interpersonal[j][i])\n",
    "        else:\n",
    "            interpersonal_mh.append(dataset_interpersonal[j][i])\n",
    "    result = sm.stats.CompareMeans.from_data(interpersonal_mh, interpersonal_mh_sw).ztest_ind()\n",
    "    #result2 = sm.stats.CompareMeans.from_data(interpersonal_mh, interpersonal_mh_sw).ttest_ind()\n",
    "    print(\"Mean (MH, SW): \", numpy.mean(interpersonal_mh), numpy.mean(interpersonal_mh_sw))\n",
    "    print(result)\n",
    "    #print(result2)\n",
    "    \n",
    "print(\"Z-test for sentiment features:\")   \n",
    "for i in range(len(dataset_sentiment[1])-1):\n",
    "    sentiment_mh = []\n",
    "    sentiment_mh_sw = []\n",
    "    for j in range(len(dataset_sentiment)):\n",
    "        if dataset_sentiment[j][len(dataset_sentiment[j])-1] == 1:\n",
    "            sentiment_mh_sw.append(dataset_sentiment[j][i])\n",
    "        else:\n",
    "            sentiment_mh.append(dataset_sentiment[j][i])\n",
    "    result = sm.stats.CompareMeans.from_data(sentiment_mh, sentiment_mh_sw).ztest_ind()\n",
    "    #result2 = sm.stats.CompareMeans.from_data(sentiment_mh, sentiment_mh_sw).ttest_ind()\n",
    "    print(\"Mean (MH, SW): \", numpy.mean(sentiment_mh), numpy.mean(sentiment_mh_sw))\n",
    "    print(result)\n",
    "    #print(result2)\n",
    "    \n",
    "# lin = ['num_difficult_words','fleisch_reading_ease', 'automated_readability_index','pronoun_fraction', 'noun_fraction',\\\n",
    "#         'verb_fraction','adverb_fraction', 'is_mh_sw']\n",
    "# meta = ['post_length', 'title_length','reddit_score', 'num_comments_to_post', 'comment_length', 'comment_score',\\\n",
    "#         'num_comment', 'is_mh_sw']\n",
    "# interpersonal = ['fp_singular_fraction', 'fp_plural_fraction', 'sp_fraction', 'tp_fraction', 'is_mh_sw']\n",
    "# sen = ['negative_score', 'neutral_score', 'positive_score', 'compound_score', 'is_mh_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print(len(mh))\n",
    "# print(len(mh_sw))\n",
    "# print(len(mh_sample))\n",
    "# print(len(mh_sample_post))\n",
    "# print(len(mh_sw_post))\n",
    "# print(len(mh_sw_comment))\n",
    "# print(len(mh_sample_comment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# write dataset to CSV file, feature_choice is parameter to decide what dataset and the name of CSV file\n",
    "def writeCSV(dataset, treatmentToken, feature_choice):\n",
    "    set1 = ['num_difficult_words','fleisch_reading_ease', 'automated_readability_index','pronoun_fraction', 'noun_fraction',\\\n",
    "            'verb_fraction','adverb_fraction', 'is_mh_sw']\n",
    "    set2 = ['post_length', 'title_length','reddit_score', 'num_comments_to_post', 'comment_length', 'comment_score',\\\n",
    "            'num_comment', 'is_mh_sw']\n",
    "    set3 = ['fp_singular_fraction', 'fp_plural_fraction', 'sp_fraction', 'tp_fraction', 'is_mh_sw']\n",
    "    set4 = ['negative_score', 'neutral_score', 'positive_score', 'compound_score', 'is_mh_sw']\n",
    "\n",
    "    \n",
    "    set6 = copy.deepcopy(set1[0:len(set1)-1])\n",
    "\n",
    "#     set5 = ['num_difficult_words','fleisch_reading_ease', 'automated_readability_index','pronoun_fraction', 'noun_fraction',\\\n",
    "#             'verb_fraction','adverb_fraction',\\\n",
    "#             'post_length', 'title_length','reddit_score', 'num_comments_to_post', 'comment_length', 'comment_score',\\\n",
    "#             'num_comment',\\\n",
    "#             'fp_singular_fraction', 'fp_plural_fraction', 'sp_fraction', 'tp_fraction',\n",
    "#             'negative_score', 'neutral_score', 'positive_score', 'compound_score', 'is_mh_sw']\n",
    "    treatment_copy = copy.deepcopy(treatmentToken)\n",
    "    set5 = treatment_copy\n",
    "    set5.append('is_mh_sw')\n",
    "\n",
    "#    set6.extend(set2[0:len(set2)-1]).extend(set3[0:len(set3)-1]).extend(set4[0:len(set4)-1]).extend(set5)\n",
    "    set6.extend(set2[0:len(set2)-1])\n",
    "    set6.extend(set3[0:len(set3)-1])\n",
    "    set6.extend(set4[0:len(set4)-1])\n",
    "    set6.extend(set5)\n",
    "\n",
    "    if feature_choice == 1:\n",
    "        feature = set1\n",
    "        filename = 'dataset_linguistic.csv'\n",
    "    elif feature_choice == 2:\n",
    "        feature = set2\n",
    "        filename = 'dataset_metadata.csv'\n",
    "    elif feature_choice == 3:\n",
    "        feature = set3\n",
    "        filename = 'dataset_interpersonal.csv'\n",
    "    elif feature_choice == 4:\n",
    "        feature = set4\n",
    "        filename = 'dataset_sentiment.csv'\n",
    "    elif feature_choice == 5:\n",
    "        feature = set5\n",
    "        filename = 'dataset_content.csv'\n",
    "    elif feature_choice == 6:\n",
    "        feature = set6\n",
    "        filename = 'dataset_full.csv'\n",
    "    else:\n",
    "        raise Exception('Invalid choice!')\n",
    "        \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        file_writer = csv.writer(file)    \n",
    "        file_writer.writerow(feature)\n",
    "        for row in dataset:\n",
    "            file_writer.writerow(row)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "writeCSV(dataset_linguistic, treatmentToken, 1)\n",
    "writeCSV(dataset_metadata, treatmentToken, 2)\n",
    "writeCSV(dataset_interpersonal, treatmentToken, 3)\n",
    "writeCSV(dataset_sentiment, treatmentToken, 4)\n",
    "writeCSV(dataset_content, treatmentToken, 5)\n",
    "writeCSV(dataset_full, treatmentToken, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
